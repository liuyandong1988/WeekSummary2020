# 多智能体强化学习——保持两车距离

[![JEvtvq.gif](https://s1.ax1x.com/2020/04/17/JEvtvq.gif)](https://imgchr.com/i/JEvtvq)
## 1. 单agentDQN控制
前车分别以恒定速度，随机速度前行，后车使用DQN控制保持车距。
1. 全局状态信息：前车的速度、位置，后车的速度，位置和两车之间的距离。连续4帧图像
2. 后车局部观测：后车的位置和速度
3. reward值
|保持距离（100）|动作|奖励|
|:--:|:--:|:-----:|
|大于100+10|加速|+1|
| |其他动作|-1|
|保持在100~110|任何动作|+10|
|小于100|减速|+1|
||其他动作|-10|
|相撞 reward = -100|||
4.实验结果，3组实验，使用全局信息前车速度恒定，全局信息前车速度随机变化，后车局部信息前车速度恒定。
[![JEzcB6.png](https://s1.ax1x.com/2020/04/17/JEzcB6.png)](https://imgchr.com/i/JEzcB6)
[![JEz6nx.png](https://s1.ax1x.com/2020/04/17/JEz6nx.png)](https://imgchr.com/i/JEz6nx)
**<font color=blue size=5 >局部观测信息是不收敛的，使用MARL方法，多车辆协同控制车队队形。</font>**


##  2.MARL
###  2.1 中心式（一个训练Model）
每个agent分别采集观测信息，合并后供给模型训练，使用联合动作空间，得到对应的reward值。
1. 实验框架
[![JVSe29.md.jpg](https://s1.ax1x.com/2020/04/17/JVSe29.md.jpg)](https://imgchr.com/i/JVSe29)
2. 实验设置

数据初始化
[![JVVnYt.md.jpg](https://s1.ax1x.com/2020/04/17/JVVnYt.md.jpg)](https://imgchr.com/i/JVVnYt)

reward值
[![JVVmFI.md.jpg](https://s1.ax1x.com/2020/04/17/JVVmFI.md.jpg)](https://imgchr.com/i/JVVmFI)

3. 实验结果
    [![JVVWp6.png](https://s1.ax1x.com/2020/04/17/JVVWp6.png)](https://imgchr.com/i/JVVWp6)
    [![JVVf1K.png](https://s1.ax1x.com/2020/04/17/JVVf1K.png)](https://imgchr.com/i/JVVf1K)
    [![JVZihq.png](https://s1.ax1x.com/2020/04/17/JVZihq.png)](https://imgchr.com/i/JVZihq)
    不同的初始距离distance：180,150,120,90,60距离变化图

 **TODO:记录速度变化的情况；不同的初始速度图像的变化情况**


**<font color=blue size=5 >缺点：（1）联合动作空间维度爆炸；（2）agent本体端无法控制；（3）系统的扩展性差；（4）teamreward设计困难</font>**


### 2.2 分布式 同时独立控制两个agent没有协同
这种控制方法属于局部可观的马尔科夫过程，系统发散。
[![JVK8mT.md.jpg](https://s1.ax1x.com/2020/04/17/JVK8mT.md.jpg)](https://imgchr.com/i/JVK8mT)
算法框架
[![JVuoFJ.png](https://s1.ax1x.com/2020/04/17/JVuoFJ.png)](https://imgchr.com/i/JVuoFJ)
[![JVuTY9.png](https://s1.ax1x.com/2020/04/17/JVuTY9.png)](https://imgchr.com/i/JVuTY9)
训练结果，reward and distance 都是发散的。
 **使用何种算法框架，可以以分布式的方法控制每一个agent，即agent的本体端有学习模型。（Value decomposed networt）**

 ### 2.3 分布式  Value decomposed networt
 VDN要求系统的价值函数线性可分解，即 team reward 值是各个agent reward的累加和。agent目标互不相关。在keep distance问题上，不满足这个假设条件。可以参看 多车辆制动问题。
[![JVlZTO.md.jpg](https://s1.ax1x.com/2020/04/17/JVlZTO.md.jpg)](https://imgchr.com/i/JVlZTO)
VDN算法框架


## TODO
1. 找个MARL分布式控制的算法；
2. 讲keep distance场景扩展到 formation control 。单车道多车辆，多车道多车辆。

 

 

 