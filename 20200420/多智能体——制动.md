# 强化学习--多车辆停车

[![JENvqA.gif](https://s1.ax1x.com/2020/04/17/JENvqA.gif)](https://imgchr.com/i/JENvqA)
使用强化学习的方法训练多个agent制动停车

## 分布式独立DQN
每个agent有自己的训练模型，训练的初始速度为（1.5, 0）。
得到的损失函数不收敛（TODO：强化学习中的损失函数收敛问题）
[![Jksd2V.png](https://s1.ax1x.com/2020/04/16/Jksd2V.png)](https://imgchr.com/i/Jksd2V)
制动停车停车距离随着episode收敛于最佳制动距离。
[![JkswvT.png](https://s1.ax1x.com/2020/04/16/JkswvT.png)](https://imgchr.com/i/JkswvT)
测试agentA以速度（2.0, 0）制动，agentB以速度（1.0, 0）制动。
[![Jksa80.png](https://s1.ax1x.com/2020/04/16/Jksa80.png)](https://imgchr.com/i/Jksa80)

## Value Decomposed Network
Q-value值分解的假设条件为，各个agent的目标独立，即team-reward是各个agent reward的线性加和。做和上述实验相同的测试，结果如下。
[![JEaa0s.png](https://s1.ax1x.com/2020/04/17/JEaa0s.png)](https://imgchr.com/i/JEaa0s)
[![JEaUmj.png](https://s1.ax1x.com/2020/04/17/JEaUmj.png)](https://imgchr.com/i/JEaUmj)
[![JEatXQ.png](https://s1.ax1x.com/2020/04/17/JEatXQ.png)](https://imgchr.com/i/JEatXQ)

VDN解决了集中式训练中，各个agent把team-reward当做自己的reward，信用分配问题 credit assignment。但其假设reward值是可以线性分解的。没有办法解决非线性reward关系和竞争场景的问题。

## TODO：
**<font color=blue size=5 >
（1）用什么方法解决非线性reward关系和竞争场景的问题；
（2）多agent协同停车场景的问题;</font>**